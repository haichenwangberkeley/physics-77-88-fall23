{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a050c03",
   "metadata": {},
   "source": [
    "# Workshop 8 Hypothesis Testing\n",
    "\n",
    "**Due by 9 pm October 17, 2023**\n",
    "\n",
    "During Wednesday's lecture, we discussed the example of interpreting the outcome of the Stern Gerlach experiment. In this exercise, you will develop the code to perform the hypothesis testing and measurement of the probability parameter $p$ (the probability for atoms to end up at the upper position after passing through the magnetic field). \n",
    "\n",
    "\n",
    "# Part-1 Hypothesis Testing\n",
    "\n",
    "\n",
    "## Visualize Binomial Function\n",
    "\n",
    "Below is a code cell used to generate the plot included in the lecture slides. Use it as an example, and produce a plot to visualize \n",
    "- a Binomial PDF where N = 4000 and p = 0.5\n",
    "- a Binomial PDF where N = 4000 and p = 0.475 \n",
    "- a Binomial PDF where N = 4000 and p = 0.525\n",
    "\n",
    "Other requirements\n",
    "- Overlay the three functions in the same plot\n",
    "- Set the Y axis to logarithmic scale (google search if you are not sure what to do here). Then turn it off to see the difference between the linear scale plot and the log scale one.\n",
    "- In a markdown cell, report the standard deviation of each of these functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Parameters\n",
    "N = 20000\n",
    "p = 0.5\n",
    "\n",
    "# Mean and standard deviation for the binomial distribution\n",
    "mu = N * p\n",
    "sigma = np.sqrt(N * p * (1-p))\n",
    "\n",
    "# Adjusted range\n",
    "k_min = int(mu - 6*sigma)\n",
    "k_max = int(mu + 6*sigma)\n",
    "\n",
    "# Values\n",
    "k = np.arange(k_min, k_max+1)\n",
    "probabilities = binom.pmf(k, N, p)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k, probabilities, 'o-', markersize=4, label=f'N={N}, p={p}')\n",
    "plt.title('Binomial PDF as a model of the Stern-Gerlach Exp.',fontsize=20)\n",
    "plt.xlabel('Number of atoms at the upper position',fontsize=20)\n",
    "plt.ylabel('Probability',fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e09adae",
   "metadata": {},
   "source": [
    "**Your answers here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eeab72",
   "metadata": {},
   "source": [
    "In one case, someone conducted the experiment and collected data for 4,000 atoms. Their observation is that k = 2023, i.e., 2023 atoms, out of 4,000 sent through the magnetic field, went up. We want to use this observation to test the hypothesis that p = 0.475. We will do this step-by-step.\n",
    "\n",
    "## Generate P.E.s\n",
    "First, we need to generate pseudo experiments from the hypothesis under test. We model the outcome of this experiment with a binomial PDF where N = 4000, and our hypothesis is p = 0.475. Our pseudo experiments are outcomes following the PDF of the hypothesis under test. The binomial PDF tells you the probability of having an outcome of k (k atoms ending up at the upper position) given N and p. We need to repeatedly draw random numbers from this binomial PDF. Each time we draw a random number from the binomial PDF, we are repeating the experiment of **sending 4000 atoms through the magnetic field** (not the experiment sending one atom through the field). Because we run pseudo experiments, we can run as many as we want. So let's do it M = 1,000,000 times. You need to complete the cell below to do the P.E. generation under the hypothesis of = 0.475, and at the end of it, you will have a numpy array to store the P.E. outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ad8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the cell below\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Parameters\n",
    "N = # Total number of trials\n",
    "p_hypothesis =   # Hypothesized value of p\n",
    "M =   # Number of pseudo-experiments\n",
    "\n",
    "# how do you plug in the three varialbes defined above?\n",
    "pe_outcomes = np.random.binomial( )\n",
    "\n",
    "# Plot a histogram of the pseudo-experiment outcomes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(pe_outcomes, bins=100, density=False, alpha=0.6, color='b', label=f'N={N}, p={p_hypothesis}')\n",
    "\n",
    "# Configure plot labels and legend\n",
    "\n",
    "plt.xlabel('Outcome in P.E.s', fontsize=14)\n",
    "plt.ylabel('Number of P.E.s', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf269cf9",
   "metadata": {},
   "source": [
    "Now that we have the P.E. outcomes, we can determine how frequent the outcomes of the hypothesis of p = 0.475 can be at least as large as the observed outcome $k = 2023$. In practice, you need to calculate the number of entries in the PE outcome array that are greater than 2023, and then take the ratio of that to the total number of P.E.s. Perform your calculation in the cell below, and summarize your result in a markdown cell.\n",
    "\n",
    "**N.B. the quantity you calculated here is known as the $p-$value, not to be confused with the p parameter in the binomial PDF. The $p-$value the probability of the outcome from your hypothesis under test to be at least as extreme as the actual observed outcome.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82755748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44c77f",
   "metadata": {},
   "source": [
    "**Summarize your result**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713c65f",
   "metadata": {},
   "source": [
    "The concept of $p$-value is closely related to the significance. $p-$value quantifies the probability of the hypothesis under test to produce an outcome at least as extreme as the observed outcome, while the significance measures the same probability but instead presents it as a number of standard deviations away from the hypothesis under test. In practice, we often **conventionally** assume the relation between the p-value and significance follow that in the Gaussian case. We can convert the p-value to significance and vice versa. You can see the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5144bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code cell will use the p_value you defined in the previous cell as input\n",
    "# What is the significance of \"rejecting\" the p=0.475 hypothesis?\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Function to convert p-value to z-score\n",
    "def p_value_to_significance(p_value):\n",
    "    return -stats.norm.ppf(p_value / 2)\n",
    "\n",
    "# Function to convert z-score to p-value\n",
    "def significance_to_p_value(z):\n",
    "    return 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "\n",
    "# Example: Convert p-value to significance\n",
    "\n",
    "significance = p_value_to_significance(p_value)\n",
    "print(f\"P-value: {p_value}, significance: {significance:.4f}\")\n",
    "\n",
    "# Example: Convert z-score to p-value\n",
    "z = 2.0  # Example z-score\n",
    "p_value_example = significance_to_p_value(z)\n",
    "print(f\"Z-score: {significance}, P-value: {p_value_example:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f157bc7",
   "metadata": {},
   "source": [
    "## Your Exercise\n",
    "\n",
    "Now that you have walked through the example of hypothesis test. You can do your own test. In this exercise, follow the above procedure to test the hypothesis of p = 0.525 using the observed outcome of k = 2023. Report the p-value of this test and the significance of rejecting the hypothesis of p=0.525 in a markdown cell. \n",
    "\n",
    "**N.B.** the hypothesis of p = 0.525 would lead to an average of k = 2100 which is greater than the observed k of 2023. In this case, the probability for you to test is the one that the outcome of the p = 0.525 hypothesis to be equal to or smaller than k = 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5123e9",
   "metadata": {},
   "source": [
    "**Summarize your results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a9714",
   "metadata": {},
   "source": [
    "# Part 2. Log Likelihood Ratio as a Test Statistic \n",
    "\n",
    "Test statistic is a numerical measure to characterize an experimental outcome. In this example, we will construct a log likelihood ratio test statistic, and we will use it to perform a hypothesis test. \n",
    "\n",
    "Building upon the part 1, let's first see how we can construct a log likelihood ratio for that experiment. A log likelihood ratio is defined as\n",
    "\n",
    "$\\large\n",
    "    \\lambda = \\mathrm{ln}\\frac{L_0}{L_1} = \\mathrm{ln}{L_0} - \\mathrm{ln}{L_1}\n",
    "$\n",
    "\n",
    "where $L_0$ and $L_1$ are the likelihood functions for two different hypotheses, $H0$ and $H1$. Often one hypothesis is referred to as a null hypothesis, and the other as an alternative hypothesis. These hypothese are chosen by you. The null hypothesis is typically the one expected, and the alternative hypothesis tends to be the one that's a bit more speculative. In this case, we can choose p = 0.5 as the null hypothesis (H0) and p = 0.525 as the alternative hypothesis (H1).\n",
    "\n",
    "## Implementing Log Likliehood and Log Likelihood Ratio\n",
    "\n",
    "For common PDFs, scipy.stats provided functions to determine the logarithmic value of the PDF/likelihood. Let's see the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Parameters\n",
    "N = 4000  # Total number of trials\n",
    "p_hypothesis = 0.475  # Hypothesized value of p\n",
    "k = 2023\n",
    "\n",
    "binomial_pmf = binom.pmf(k, N, p_hypothesis)\n",
    "\n",
    "# Print type, size, and values of binomial_pmf\n",
    "print(\"Type of binomial_pmf:\", type(binomial_pmf))\n",
    "print(\"Size (shape) of binomial_pmf:\", binomial_pmf.shape)\n",
    "print(\"Values of binomial_pmf:\")\n",
    "print(binomial_pmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bd64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the logarithm of the binomial PMF (logPMF)\n",
    "log_binomial_pmf = np.log(binomial_pmf)\n",
    "\n",
    "# Print type, size, and values of log_binomial_pmf\n",
    "print(\"Type of log_binomial_pmf:\", type(log_binomial_pmf))\n",
    "print(\"Size (shape) of log_binomial_pmf:\", log_binomial_pmf.shape)\n",
    "print(\"Values of log_binomial_pmf:\")\n",
    "print(log_binomial_pmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbede091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we directly use the logpmf function of scipy.stats\n",
    "\n",
    "log_binomial_pmf = binom.logpmf(k, N, p_hypothesis)\n",
    "\n",
    "# Print type, size, and values of log_binomial_pmf\n",
    "print(\"Type of log_binomial_pmf:\", type(log_binomial_pmf))\n",
    "print(\"Size (shape) of log_binomial_pmf:\", log_binomial_pmf.shape)\n",
    "print(\"Values of log_binomial_pmf:\")\n",
    "print(log_binomial_pmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e260534",
   "metadata": {},
   "source": [
    "Now, let's generalize this to an experiment with multiple observed outcomes. For example, one experiment consists of two independently conducted Stern Gerlach experiments, A and B (just like the one explained during the lecture), the observed outcome is $k_A = 2023$ and $k_B$ = 1995. \n",
    "\n",
    "The likelihood function for this experiment is \n",
    "\n",
    "$\n",
    "\\large L(k_A, k_B, N_A, N_b, p) = f_{binomial}(k_A, N_A, p)*f_{binomial}(k_B, N_B,p) \n",
    "$\n",
    "\n",
    "and the log likelihood function is then \n",
    "\n",
    "$\n",
    "\\large \\mathrm{ln}{L(k_A, k_B, N_A, N_b, p)} = \\mathrm{ln}(f_{binomial}(k_A, N_A, p)) + \\mathrm{ln}(f_{binomial}(k_B, N_B, p))\n",
    "$\n",
    "\n",
    "Let's implement the log likleihood of this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85731e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the observed outcome is stored as a 1D array with a shape of (2,)\n",
    "k = np.array([2023,1995])\n",
    "\n",
    "# what are the type and shape for k, N, and p_hypothesis?\n",
    "# what are the type and shape for the returning argument?\n",
    "log_binomial_pmf = binom.logpmf(k, N, p_hypothesis)\n",
    "\n",
    "\n",
    "# Print type, size, and values of log_binomial_pmf\n",
    "print(\"Type of log_binomial_pmf:\", type(log_binomial_pmf))\n",
    "print(\"Size (shape) of log_binomial_pmf:\", log_binomial_pmf.shape)\n",
    "print(\"Values of log_binomial_pmf:\")\n",
    "print(log_binomial_pmf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802d1ef",
   "metadata": {},
   "source": [
    "As you can see the returning argument of the line `log_binomial_pmf = binom.logpmf(k, N, p_hypothesis)` is not a scalar. It has a shape of (2,). The log likelihood is a single-valued quantity. The two entries in `log_binomial_pmf` are corresponding to the two terms $\\mathrm{ln}(f_{binomial}(k_A, N_A, p))$ and$ \\mathrm{ln}(f_{binomial}(k_B, N_B, p))$. You need to add them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01071be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = log_binomial_pmf.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4f07e",
   "metadata": {},
   "source": [
    "Now develop your code to calculate the log likelihood ratio of $\\lambda = -2\\mathrm{ln}\\frac{L_0}{L_1}$. Here you need to calculate the log likelihood of the observed outcome (kA and kB) for the null hypothesis and the alternative hypothesis, separately. Then their difference is the log likelihood ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91002af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# Calculate the log likelihood for the null hypothesis (p = 0.5)\n",
    "LLH0 = \n",
    "# Calculate the log likelihood for the alternative hypothesis (p = 0.525)\n",
    "LLH1 = \n",
    "\n",
    "# Calculate the log likelihood ratio (LLR) between H0 and H1\n",
    "LLR = LLH0 - LLH1\n",
    "\n",
    "# Print the log likelihood values and log likelihood ratio\n",
    "print(\"Log Likelihood for H0 (p = 0.5):\", LLH0)\n",
    "print(\"Log Likelihood for H1 (p = 0.525):\", LLH1)\n",
    "print(\"Log Likelihood Ratio (LLR) between H0 and H1:\", LLR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f00ef",
   "metadata": {},
   "source": [
    "# Part 3 Hypothesis Test using the log likelihood ratio\n",
    "\n",
    "Now that you can generate P.E.s and you can construct a log likelihood ratio, use them to perform a hypothesis test. Our experiment consists of five group of students who performed statistically independent data collection of the Stern-Gerlach experiment. Their observed number atoms at the upper position are 2005, 1990, 1991, 2015, 1995. For these five independent measurements, their total numbers of atoms sent through the magentic field are also different. They are 4002, 3995,3994,4004,4005. Given this observation of five independent measurement resutlts, what are the p-value and significance of the hypothesis of p = 0.51?\n",
    "\n",
    "Here is a roadmap for you to implement this test:\n",
    "\n",
    "1. each experiment has five statistically independent observations, and as such, you should use arrays of shape (1,5) to store the observed and P.E. outcomes, and the total number of atoms. These arrays are shaped to (1,5) so that each entry on axis 0 would be corresponding to one experiment, and each entry on axis 1 would be corresponding to one of the five observations made within in one experiment. Later, when you generate M number of P.E.s, the P.E.s can be stored in an array of shape (M,5) where you M entries on axis 0 corresponding to the M P.E.s.\n",
    "\n",
    "2. define a function to calculate the log likelihood for a given outcome, total number of atoms, and hypothesized value of p. This function can take the arrays of shape (1,5) as input arguments. Make sure the returning arugment is a scalar. For the observed outcome, identify the value of parameter $p$ that maximized the log likelihood. \n",
    "\n",
    "3. Generate P.E.s using the hypothesis under test. In this case, generate P.E.s with p = 0.51. You want to see how frequent outcomes of p = 0.51 become as extreme as the observed outcome.\n",
    "\n",
    "4. For each P.E. outcome, calculate their log likelihod raito, with H0 being p=0.5, and H1 being p = 0.51. To help you understand the P.E. outcomes, draw the log likelihood ratio distribution of these P.E.s, and also use a vertical line to indicate the log likelihood ratio value of the acutal observed outcome. \n",
    "\n",
    "5. Determine the p-value and significance. Calculate the fraction of P.E.s that are more extreme than the observed outcome. Integrate the tail area under the curve starting from the log likelihood ratio value of the actual observed outcome. The p-value is this area over the total area under the curve. Convert your p-value to significance. Report both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa303f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for step 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
